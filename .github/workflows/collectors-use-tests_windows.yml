name: Test Windows [snowpark_checkpoints_collector]

on:
  workflow_dispatch:
    inputs:
      python-version:
        description: 'Python version to test'
        required: false
        default: '3.9'
        type: choice
        options:
        - '3.9'
        - '3.10'
        - '3.11'

permissions:
  contents: read
  pull-requests: write

jobs:
  test:
    name: Test ${{ matrix.download_name }}-${{ matrix.python-version }}-${{ matrix.cloud-provider }}
    runs-on: ${{ matrix.os }}
    env:
      SNOWFLAKE_CONNECTIONS_MYCONNECTION_AUTHENTICATOR: SNOWFLAKE_JWT
      SNOWFLAKE_CONNECTIONS_MYCONNECTION_DATABASE: ${{ secrets.SNOWFLAKE_DATABASE }}
      SNOWFLAKE_CONNECTIONS_MYCONNECTION_SCHEMA: ${{ secrets.SNOWFLAKE_SCHEMA }}
      SNOWFLAKE_CONNECTIONS_MYCONNECTION_WAREHOUSE: ${{ secrets.SNOWFLAKE_WH }}
      SNOWFLAKE_CONNECTIONS_MYCONNECTION_ROLE: ${{ secrets.SNOWFLAKE_ROLE }}
      SNOWFLAKE_CONNECTIONS_MYCONNECTION_USER: ${{ secrets.SNOWFLAKE_USER }}
      SNOWFLAKE_CONNECTIONS_MYCONNECTION_ACCOUNT: ${{ secrets.SNOWFLAKE_ACCOUNT }}
      SNOWFLAKE_CONNECTIONS_MYCONNECTION_PRIVATE_KEY_RAW: ${{ secrets.SNOWFLAKE_PRIVATE_KEY_RAW }}
      SNOWPARK_CHECKPOINTS_TELEMETRY_TESTING: "true"
      HADOOP_HOME: "${{ github.workspace }}\\hadoop"
      HADOOP_VERSION: "3.3.6"
      JAVA_VERSION: "21"
      PYSPARK_VERSION_WINDOWS: "==3.5.5"
      # PySpark Windows compatibility settings
      SPARK_LOCAL_IP: "127.0.0.1"
      SPARK_LOCAL_HOSTNAME: "localhost"

    strategy:
      fail-fast: false
      matrix:
        include:
          - os: windows-latest
            python-version: ${{ github.event.inputs.python-version || '3.9' }}
            cloud-provider: aws
            snow_cli_version: "2.8.2"
            download_name: windows

    steps:
      - name: Check out the repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}

      - name: Set up Java
        uses: actions/setup-java@v4
        with:
          distribution: "temurin"
          java-version: ${{ env.JAVA_VERSION }}
          check-latest: true
      - run: java --version

      - name: Generate timestamp
        shell: pwsh
        id: timestamp
        run: |
          $timestamp = Get-Date -Format yyyyMMddHHmmssfff
          $schemaId = "${{ secrets.SNOWFLAKE_SCHEMA }}_${{ github.run_id }}_${{ github.run_number }}_$timestamp"
          Add-Content -Path $Env:GITHUB_ENV -Value "SNOWFLAKE_CONNECTIONS_MYCONNECTION_SCHEMA_ID_WIN=$schemaId"

      - name: Display Python version
        shell: pwsh
        run: |
          python -c "import sys; print(sys.version)"
          python ../.github/scripts/py_show_env.py
        working-directory: ./snowpark-checkpoints-collectors

      - name: Enable long paths
        shell: pwsh
        run: |
          Write-Output "Enabling long paths on Windows..."
          Set-ItemProperty -Path "HKLM:\SYSTEM\CurrentControlSet\Control\FileSystem" -Name "LongPathsEnabled" -Value 1
          Write-Output "Long paths have been enabled."

      - name: Download Winutils
        shell: pwsh
        run: |
          $winutilsRepo = "https://github.com/cdarlint/winutils"
          $hadoopDir = "hadoop-${{ env.HADOOP_VERSION }}/bin"
          $tempDir = "$env:RUNNER_TEMP\hadoop_bin"

          New-Item -ItemType Directory -Force -Path $tempDir | Out-Null

          try {
            Invoke-WebRequest -Uri "$winutilsRepo/archive/master.zip" -OutFile "$tempDir\winutils.zip"
            Expand-Archive -Path "$tempDir\winutils.zip" -DestinationPath $tempDir -Force

            $sourceDir = Join-Path -Path $tempDir -ChildPath "winutils-master\$hadoopDir"
            $destinationDir = "${{ env.HADOOP_HOME }}\bin"

            New-Item -ItemType Directory -Force -Path $destinationDir | Out-Null
            Get-ChildItem -Path $sourceDir | Move-Item -Destination $destinationDir -Force
            $winutilsPath = Join-Path -Path $destinationDir -ChildPath "winutils.exe"

            if (-Not (Test-Path $winutilsPath)) {
              throw "winutils.exe not found in $destinationDir"
            }

            Write-Output "Winutils successfully installed at $winutilsPath"
          }
          catch {
            Write-Error "Winutils download or installation failed: $_"
            exit 1
          }

      - name: Configure Hadoop
        shell: pwsh
        run: |
          Write-Output "Configuring Hadoop environment variables"
          echo "HADOOP_HOME=${{ env.HADOOP_HOME }}" | Out-File -FilePath $env:GITHUB_ENV -Append
          $hadoopBinPath = "${{ env.HADOOP_HOME }}\bin"
          echo "PATH=$env:PATH;$hadoopBinPath" | Out-File -FilePath $env:GITHUB_ENV -Append
          Write-Output "HADOOP_HOME set to: ${{ env.HADOOP_HOME }}"
          Write-Output "Hadoop bin added to PATH"

      - name: Verify Hadoop
        shell: pwsh
        run: |
          try {
            $hadoopVersion = & "${{ env.HADOOP_HOME }}\bin\hadoop" version
            Write-Output "Hadoop version verified: $hadoopVersion"
          }
          catch {
            Write-Error "Hadoop version check failed: $_"
            exit 1
          }

          Write-Output "Hadoop Home Contents:"
          Get-ChildItem -Path "${{ env.HADOOP_HOME }}" -Recurse | Select-Object FullName, Length | Format-Table -AutoSize

      - name: Test Hadoop Configuration
        shell: pwsh
        run: |
          try {
            & "${{ env.HADOOP_HOME }}\bin\hdfs" dfs -mkdir -p /test

            $testList = & "${{ env.HADOOP_HOME }}\bin\hdfs" dfs -ls /
            Write-Output "HDFS Basic Test Passed"
            Write-Output "Directory Listing: $testList"
          }
          catch {
            Write-Error "Hadoop HDFS test failed: $_"
            exit 1
          }

      - name: Set up Snowflake CLI
        uses: Snowflake-Labs/snowflake-cli-action@c2274560296ae95dd86b511d288d4657006235f2
        with:
          cli-version: ${{ matrix.snow_cli_version }}
          default-config-file-path: ".github/config/config.toml"

      - name: Set up key
        shell: pwsh
        run: |
          $ParentDir = Split-Path -Parent ${{ github.workspace }}
          $PrivateKeyFile = Join-Path $ParentDir '.ssh\key.p8'

          New-Item -ItemType Directory -Force -Path "$ParentDir\.ssh" | Out-Null
          Set-Content -Path $PrivateKeyFile -Value $Env:SNOWFLAKE_CONNECTIONS_MYCONNECTION_PRIVATE_KEY_RAW
          Write-Host "Private key file created at: $PrivateKeyFile"

          icacls $PrivateKeyFile /inheritance:r /grant:r "$($Env:USERNAME):(R,W)"
          icacls $PrivateKeyFile /grant:r *S-1-5-32-544:RW

      - name: Set up Connection Snowflake CLI
        shell: pwsh
        run: |
          $PARENT_DIR = Split-Path -Parent "${{ github.workspace }}"
          $PRIVATE_KEY_FILE = (Join-Path (Join-Path $PARENT_DIR '.ssh') 'key.p8') -replace '\\', '\\\\'
          $CONFIG_FILE=".github\config\config.toml"
          $SNOWFLAKE_CONFIG_DIR = "$env:USERPROFILE\.snowflake"
          $SNOWFLAKE_CONFIG_PATH = "$SNOWFLAKE_CONFIG_DIR\config.toml"

          Write-Output "SNOWFLAKE_CONFIG_DIR:  $SNOWFLAKE_CONFIG_DIR"
          Write-Output "SNOWFLAKE_CONFIG_PATH: $SNOWFLAKE_CONFIG_PATH"

          New-Item -ItemType Directory -Force -Path $SNOWFLAKE_CONFIG_DIR

          Set-Content -Path $CONFIG_FILE -Value "[connections.myconnection]"
          Add-Content -Path $CONFIG_FILE -Value "authenticator = `"$env:SNOWFLAKE_CONNECTIONS_MYCONNECTION_AUTHENTICATOR`""
          Add-Content -Path $CONFIG_FILE -Value "schema = `"$env:SNOWFLAKE_CONNECTIONS_MYCONNECTION_SCHEMA`""
          Add-Content -Path $CONFIG_FILE -Value "account = `"$env:SNOWFLAKE_CONNECTIONS_MYCONNECTION_ACCOUNT`""
          Add-Content -Path $CONFIG_FILE -Value "user = `"$env:SNOWFLAKE_CONNECTIONS_MYCONNECTION_USER`""
          Add-Content -Path $CONFIG_FILE -Value "database = `"$env:SNOWFLAKE_CONNECTIONS_MYCONNECTION_DATABASE`""
          Add-Content -Path $CONFIG_FILE -Value "warehouse = `"$env:SNOWFLAKE_CONNECTIONS_MYCONNECTION_WAREHOUSE`""
          Add-Content -Path $CONFIG_FILE -Value "role = `"$env:SNOWFLAKE_CONNECTIONS_MYCONNECTION_ROLE`""
          Add-Content -Path $CONFIG_FILE -Value "private_key_file = `"$PRIVATE_KEY_FILE`""

          Add-Content -Path $CONFIG_FILE -Value "[connections.integration]"
          Add-Content -Path $CONFIG_FILE -Value "authenticator = `"$env:SNOWFLAKE_CONNECTIONS_MYCONNECTION_AUTHENTICATOR`""
          Add-Content -Path $CONFIG_FILE -Value "schema = `"$env:SNOWFLAKE_CONNECTIONS_MYCONNECTION_SCHEMA_ID_WIN`""
          Add-Content -Path $CONFIG_FILE -Value "account = `"$env:SNOWFLAKE_CONNECTIONS_MYCONNECTION_ACCOUNT`""
          Add-Content -Path $CONFIG_FILE -Value "user = `"$env:SNOWFLAKE_CONNECTIONS_MYCONNECTION_USER`""
          Add-Content -Path $CONFIG_FILE -Value "database = `"$env:SNOWFLAKE_CONNECTIONS_MYCONNECTION_DATABASE`""
          Add-Content -Path $CONFIG_FILE -Value "warehouse = `"$env:SNOWFLAKE_CONNECTIONS_MYCONNECTION_WAREHOUSE`""
          Add-Content -Path $CONFIG_FILE -Value "role = `"$env:SNOWFLAKE_CONNECTIONS_MYCONNECTION_ROLE`""
          Add-Content -Path $CONFIG_FILE -Value "private_key_file = `"$PRIVATE_KEY_FILE`""

          Copy-Item -Path $CONFIG_FILE -Destination $SNOWFLAKE_CONFIG_PATH
          icacls $PRIVATE_KEY_FILE /inheritance:r /grant:r "$($env:USERNAME):(R)"
          icacls $SNOWFLAKE_CONFIG_PATH /inheritance:r /grant:r "$($env:USERNAME):(R)"

          Write-Output "Snowflake configuration at $SNOWFLAKE_CONFIG_PATH"
          Get-Content $SNOWFLAKE_CONFIG_PATH
        env:
          SNOWFLAKE_CONNECTIONS_MYCONNECTION_SCHEMA_ID_WIN: ${{ env.SNOWFLAKE_CONNECTIONS_MYCONNECTION_SCHEMA_ID_WIN }}

      - name: Set up Default Snowflake CLI
        shell: pwsh
        run: |
          $ErrorActionPreference = "Stop"
          snow --info
          snow --version
          snow connection test -c myconnection
          snow connection set-default myconnection
          snow sql -q "Select current_organization_name(); SELECT CURRENT_DATABASE(), CURRENT_SCHEMA();" -c myconnection
          snow connection list

          $database = $env:SNOWFLAKE_CONNECTIONS_MYCONNECTION_DATABASE
          $schemaId = $env:SNOWFLAKE_CONNECTIONS_MYCONNECTION_SCHEMA_ID_WIN

          Write-Output "Database: $database"
          Write-Output "Schema ID: $schemaId"
          $query = "CREATE SCHEMA IF NOT EXISTS $database.`"$schemaId`" WITH MANAGED ACCESS;"

          Write-Output "Executing query: $query"
          snow sql -q $query -c myconnection

          snow connection test -c integration
          snow connection set-default integration
          snow connection list
        env:
          SNOWFLAKE_CONNECTIONS_MYCONNECTION_DATABASE: "${{ env.SNOWFLAKE_CONNECTIONS_MYCONNECTION_DATABASE }}"
          SNOWFLAKE_CONNECTIONS_MYCONNECTION_SCHEMA_ID_WIN: "${{ env.SNOWFLAKE_CONNECTIONS_MYCONNECTION_SCHEMA_ID_WIN }}"

      - name: Install required tools
        shell: pwsh
        run: |
          python -m pip install --upgrade pip -q
          # Install project dependencies first without pyspark
          python -m pip install ".[development]" --no-deps -q
          # Install dependencies manually excluding pyspark
          pip install "snowflake-snowpark-python>=1.23.0" snowflake-connector-python "pandera[io]==0.20.4" -q
          pip install pytest>=8.3.3 pytest-cov>=6.0.0 coverage>=7.6.7 twine==5.1.1 hatchling==1.25.0 -q
          pip install pre-commit>=4.0.1 setuptools>=70.0.0 pyarrow>=18.0.0 deepdiff>=8.0.0 certifi==2025.1.31 -q
          pip install hatch pyyaml frictionless requests -q
          # Force install PySpark 3.5.5 for Windows compatibility (avoid 4.0+ errors)
          pip install pyspark==3.5.5 py4j==0.10.9.7 -q
          # Install additional Windows-specific PySpark dependencies
          pip install pywin32 -q
          # Configure hatch to use the correct Python version
          hatch config set dirs.env.virtual .hatch
          hatch config set template.licenses.headers false

          # Ensure hatch uses the same Python version as PySpark
          $env:HATCH_PYTHON = $env:PYSPARK_PYTHON

          pip list
        working-directory: ./snowpark-checkpoints-collectors

      - name: Display Host Info
        shell: pwsh
        run: |
          python ../.github/scripts/py_show_host.py
        working-directory: ./snowpark-checkpoints-collectors

      - name: Verify PySpark Installation
        shell: pwsh
        run: |
          Write-Host "Verifying PySpark installation and version..."
          python -c "import pyspark; print(f'PySpark version: {pyspark.__version__}')"
          python -c "from pyspark.sql import SparkSession; spark = SparkSession.builder.appName('version_check').getOrCreate(); print(f'Spark version: {spark.version}'); spark.stop()"
        working-directory: ./snowpark-checkpoints-collectors

      - name: Set PySpark
        shell: pwsh
        run: |
          $pythonPath = (Get-Command python).Source
          Write-Host "Python Path: $pythonPath"
          echo "PYSPARK_PYTHON=$pythonPath" >> $env:GITHUB_ENV
          echo "PYSPARK_DRIVER_PYTHON=$pythonPath" >> $env:GITHUB_ENV
          # Additional PySpark configuration for Windows compatibility
          echo "PYSPARK_SUBMIT_ARGS=--master local[*] --conf spark.sql.adaptive.enabled=false --conf spark.serializer=org.apache.spark.serializer.KryoSerializer --conf spark.driver.memory=2g --conf spark.executor.memory=2g --conf spark.driver.host=127.0.0.1 --conf spark.driver.bindAddress=127.0.0.1 --conf spark.python.worker.reuse=true pyspark-shell" >> $env:GITHUB_ENV
          # Set Python path for PySpark workers
          echo "PYTHONPATH=$pythonPath" >> $env:GITHUB_ENV
          # Set network binding for Windows
          echo "SPARK_LOCAL_IP=127.0.0.1" >> $env:GITHUB_ENV
          echo "SPARK_LOCAL_HOSTNAME=localhost" >> $env:GITHUB_ENV

      - name: Verify PySpark Configuration
        shell: pwsh
        run: |
          python --version
          Write-Host "PYSPARK_PYTHON: $env:PYSPARK_PYTHON"
          Write-Host "PYSPARK_DRIVER_PYTHON: $env:PYSPARK_DRIVER_PYTHON"
          Write-Host "PYSPARK_SUBMIT_ARGS: $env:PYSPARK_SUBMIT_ARGS"
          Write-Host "PYTHONPATH: $env:PYTHONPATH"
          Write-Host "SPARK_LOCAL_IP: $env:SPARK_LOCAL_IP"
          Write-Host "SPARK_LOCAL_HOSTNAME: $env:SPARK_LOCAL_HOSTNAME"
          Write-Host "Verifying PySpark installation and version..."
          python -c "import pyspark; print(f'PySpark version: {pyspark.__version__}')"

      - name: Test PySpark Configuration
        shell: pwsh
        run: |
          Write-Host "Testing PySpark configuration before running full test suite..."
          Write-Host "PYSPARK_PYTHON: $env:PYSPARK_PYTHON"
          Write-Host "PYSPARK_DRIVER_PYTHON: $env:PYSPARK_DRIVER_PYTHON"
          Write-Host "PYSPARK_SUBMIT_ARGS: $env:PYSPARK_SUBMIT_ARGS"
          Write-Host "PYTHONPATH: $env:PYTHONPATH"

          # Create a simplified test script
          "import sys, os" | Out-File -FilePath "test_pyspark.py" -Encoding utf8
          "print('Python version:', sys.version)" | Add-Content "test_pyspark.py"
          "print('Python executable:', sys.executable)" | Add-Content "test_pyspark.py"
          "print('PYSPARK_PYTHON:', os.environ.get('PYSPARK_PYTHON', 'Not set'))" | Add-Content "test_pyspark.py"
          "print('PYSPARK_DRIVER_PYTHON:', os.environ.get('PYSPARK_DRIVER_PYTHON', 'Not set'))" | Add-Content "test_pyspark.py"
          "" | Add-Content "test_pyspark.py"
          "from pyspark.sql import SparkSession" | Add-Content "test_pyspark.py"
          "from pyspark.conf import SparkConf" | Add-Content "test_pyspark.py"
          "" | Add-Content "test_pyspark.py"
          "conf = SparkConf()" | Add-Content "test_pyspark.py"
          "conf.set('spark.sql.adaptive.enabled', 'false')" | Add-Content "test_pyspark.py"
          "conf.set('spark.serializer', 'org.apache.spark.serializer.KryoSerializer')" | Add-Content "test_pyspark.py"
          "conf.set('spark.driver.memory', '2g')" | Add-Content "test_pyspark.py"
          "conf.set('spark.executor.memory', '2g')" | Add-Content "test_pyspark.py"
          "conf.set('spark.python.worker.reuse', 'true')" | Add-Content "test_pyspark.py"
          "conf.set('spark.driver.host', '127.0.0.1')" | Add-Content "test_pyspark.py"
          "conf.set('spark.driver.bindAddress', '127.0.0.1')" | Add-Content "test_pyspark.py"
          "" | Add-Content "test_pyspark.py"
          "spark = SparkSession.builder.appName('python_version_test').config(conf=conf).getOrCreate()" | Add-Content "test_pyspark.py"
          "df = spark.range(5)" | Add-Content "test_pyspark.py"
          "result = df.collect()" | Add-Content "test_pyspark.py"
          "print('Test successful:', len(result), 'rows')" | Add-Content "test_pyspark.py"
          "print('Spark version:', spark.version)" | Add-Content "test_pyspark.py"
          "spark.stop()" | Add-Content "test_pyspark.py"

          # Show the created file for debugging
          Write-Host "Created test script:"
          Get-Content "test_pyspark.py"

          # Run the test script
          python test_pyspark.py

          # Clean up
          Remove-Item "test_pyspark.py" -ErrorAction SilentlyContinue
        working-directory: ./snowpark-checkpoints-collectors

      - name: Configure Windows for PySpark
        shell: pwsh
        run: |
          Write-Host "Configuring Windows environment for PySpark..."

          # Disable Windows Firewall temporarily for localhost connections
          Write-Host "Configuring Windows Firewall for localhost connections..."
          try {
            netsh advfirewall set allprofiles state off
            Write-Host "Windows Firewall disabled for PySpark testing"
          } catch {
            Write-Host "Could not disable firewall, continuing..."
          }

          # Configure hosts file to ensure localhost resolution
          Write-Host "Ensuring localhost resolution..."
          $hostsFile = "$env:SystemRoot\System32\drivers\etc\hosts"
          $hostsContent = Get-Content $hostsFile -ErrorAction SilentlyContinue
          if ($hostsContent -notcontains "127.0.0.1 localhost") {
            Add-Content -Path $hostsFile -Value "127.0.0.1 localhost" -ErrorAction SilentlyContinue
          }

          # Set additional environment variables for PySpark Windows compatibility
          echo "PYTHONIOENCODING=utf-8" >> $env:GITHUB_ENV
          echo "PYTHONUNBUFFERED=1" >> $env:GITHUB_ENV

          Write-Host "Windows configuration for PySpark completed"

      - name: Network Diagnostics
        shell: pwsh
        run: |
          Write-Host "Running network diagnostics for PySpark..."

          # Test localhost connectivity
          Write-Host "Testing localhost connectivity..."
          try {
            Test-NetConnection -ComputerName "127.0.0.1" -Port 80 -InformationLevel Quiet
            Write-Host "Localhost connectivity: OK"
          } catch {
            Write-Host "Localhost connectivity: Issues detected"
          }

          # Check available ports
          Write-Host "Checking available ports..."
          $availablePorts = @()
          foreach ($port in 4040..4050) {
            $connection = Test-NetConnection -ComputerName "127.0.0.1" -Port $port -InformationLevel Quiet -WarningAction SilentlyContinue
            if (-not $connection) {
              $availablePorts += $port
            }
          }
          Write-Host "Available ports: $($availablePorts -join ', ')"

          # Show network configuration
          Write-Host "Network interfaces:"
          Get-NetAdapter | Where-Object {$_.Status -eq "Up"} | Select-Object Name, InterfaceDescription, LinkSpeed

      - name: Configure PySpark for Tests
        shell: pwsh
        run: |
          Write-Host "Configuring PySpark environment for actual test execution..."

          # Set additional Spark configuration for Windows compatibility
          $env:SPARK_LOCAL_DIRS = "${{ github.workspace }}\temp\spark"
          $env:SPARK_WORKER_DIR = "${{ github.workspace }}\temp\spark\worker"
          $env:SPARK_LOG_DIR = "${{ github.workspace }}\temp\spark\logs"

          # Create necessary directories
          New-Item -ItemType Directory -Force -Path "${{ github.workspace }}\temp\spark" | Out-Null
          New-Item -ItemType Directory -Force -Path "${{ github.workspace }}\temp\spark\worker" | Out-Null
          New-Item -ItemType Directory -Force -Path "${{ github.workspace }}\temp\spark\logs" | Out-Null

          # Set environment variables for subsequent steps
          echo "SPARK_LOCAL_DIRS=${{ github.workspace }}\temp\spark" >> $env:GITHUB_ENV
          echo "SPARK_WORKER_DIR=${{ github.workspace }}\temp\spark\worker" >> $env:GITHUB_ENV
          echo "SPARK_LOG_DIR=${{ github.workspace }}\temp\spark\logs" >> $env:GITHUB_ENV

          # Override PySpark submit args with more conservative settings for tests
          echo "PYSPARK_SUBMIT_ARGS=--master local[1] --conf spark.sql.adaptive.enabled=false --conf spark.serializer=org.apache.spark.serializer.KryoSerializer --conf spark.driver.memory=1g --conf spark.executor.memory=1g --conf spark.driver.host=127.0.0.1 --conf spark.driver.bindAddress=127.0.0.1 --conf spark.python.worker.reuse=false --conf spark.sql.execution.arrow.pyspark.enabled=false --conf spark.sql.adaptive.coalescePartitions.enabled=false pyspark-shell" >> $env:GITHUB_ENV

          Write-Host "PySpark environment configured for tests"

      - name: Final PySpark Test Before Main Tests
        shell: pwsh
        run: |
          Write-Host "Running final PySpark verification before main test suite..."

          # Create a comprehensive test script
          "import os" | Out-File -FilePath "final_pyspark_test.py" -Encoding utf8
          "import sys" | Add-Content "final_pyspark_test.py"
          "print('Python executable:', sys.executable)" | Add-Content "final_pyspark_test.py"
          "print('PYSPARK_PYTHON:', os.environ.get('PYSPARK_PYTHON', 'Not set'))" | Add-Content "final_pyspark_test.py"
          "" | Add-Content "final_pyspark_test.py"
          "from pyspark.sql import SparkSession" | Add-Content "final_pyspark_test.py"
          "from pyspark.conf import SparkConf" | Add-Content "final_pyspark_test.py"
          "" | Add-Content "final_pyspark_test.py"
          "conf = SparkConf()" | Add-Content "final_pyspark_test.py"
          "conf.set('spark.master', 'local[1]')" | Add-Content "final_pyspark_test.py"
          "conf.set('spark.sql.adaptive.enabled', 'false')" | Add-Content "final_pyspark_test.py"
          "conf.set('spark.driver.memory', '1g')" | Add-Content "final_pyspark_test.py"
          "conf.set('spark.executor.memory', '1g')" | Add-Content "final_pyspark_test.py"
          "conf.set('spark.driver.host', '127.0.0.1')" | Add-Content "final_pyspark_test.py"
          "conf.set('spark.driver.bindAddress', '127.0.0.1')" | Add-Content "final_pyspark_test.py"
          "conf.set('spark.python.worker.reuse', 'false')" | Add-Content "final_pyspark_test.py"
          "conf.set('spark.sql.execution.arrow.pyspark.enabled', 'false')" | Add-Content "final_pyspark_test.py"
          "" | Add-Content "final_pyspark_test.py"
          "try:" | Add-Content "final_pyspark_test.py"
          "    spark = SparkSession.builder.appName('final_test').config(conf=conf).getOrCreate()" | Add-Content "final_pyspark_test.py"
          "    df = spark.range(10)" | Add-Content "final_pyspark_test.py"
          "    count = df.count()" | Add-Content "final_pyspark_test.py"
          "    print('DataFrame count test:', count, 'rows')" | Add-Content "final_pyspark_test.py"
          "    result = df.collect()" | Add-Content "final_pyspark_test.py"
          "    print('Collect operation test:', len(result), 'rows collected')" | Add-Content "final_pyspark_test.py"
          "    spark.stop()" | Add-Content "final_pyspark_test.py"
          "    print('Final PySpark test completed successfully')" | Add-Content "final_pyspark_test.py"
          "except Exception as e:" | Add-Content "final_pyspark_test.py"
          "    print('Final PySpark test failed:', str(e))" | Add-Content "final_pyspark_test.py"
          "    import traceback" | Add-Content "final_pyspark_test.py"
          "    traceback.print_exc()" | Add-Content "final_pyspark_test.py"
          "    sys.exit(1)" | Add-Content "final_pyspark_test.py"

          # Run the test
          python final_pyspark_test.py

          # Clean up
          Remove-Item "final_pyspark_test.py" -ErrorAction SilentlyContinue
        working-directory: ./snowpark-checkpoints-collectors

      - name: Run tests
        shell: pwsh
        run: |
          Write-Host "Current working directory: $PWD"
          Write-Host "Python executable being used: $(Get-Command python).Source"
          python --version
          Write-Host "PYSPARK_PYTHON: $env:PYSPARK_PYTHON"
          Write-Host "PYSPARK_DRIVER_PYTHON: $env:PYSPARK_DRIVER_PYTHON"
          Write-Host "PYSPARK_SUBMIT_ARGS: $env:PYSPARK_SUBMIT_ARGS"
          Write-Host "PYTHONIOENCODING: $env:PYTHONIOENCODING"
          Write-Host "PYTHONUNBUFFERED: $env:PYTHONUNBUFFERED"
          Write-Host "SPARK_LOCAL_DIRS: $env:SPARK_LOCAL_DIRS"
          Write-Host "SPARK_WORKER_DIR: $env:SPARK_WORKER_DIR"
          Write-Host "SPARK_LOG_DIR: $env:SPARK_LOG_DIR"

          # Configure hatch to use the correct Python version
          hatch config set dirs.env.virtual .hatch
          hatch config set template.licenses.headers false

          # Ensure hatch uses the same Python version as PySpark
          $env:HATCH_PYTHON = $env:PYSPARK_PYTHON

          # Set additional PySpark environment variables for test execution
          $env:SPARK_CONF_DIR = "${{ github.workspace }}"
          $env:SPARK_NO_DAEMONIZE = "1"
          $env:SPARK_DRIVER_MEMORY = "1g"
          $env:SPARK_EXECUTOR_MEMORY = "1g"

          # Get the Python version for hatch matrix
          $PYTHON_VERSION = "${{ matrix.python-version }}"
          Write-Host "Using Python version: $PYTHON_VERSION"

          # Use hatch with matrix environment directly
          Write-Host "Running tests with Python $PYTHON_VERSION"
          switch ($PYTHON_VERSION) {
            "3.9" {
              $HATCH_ENV = "test_all.py3.9"
            }
            "3.10" {
              $HATCH_ENV = "test_all.py3.10"
            }
            "3.11" {
              $HATCH_ENV = "test_all.py3.11"
            }
            default {
              $HATCH_ENV = "test"
            }
          }

          Write-Host "Using hatch environment: $HATCH_ENV"
          hatch run "$HATCH_ENV`:check"
          hatch run "$HATCH_ENV`:coverage"
        env:
          PYTHON_VERSION: ${{ matrix.python-version }}
          PYTEST_ADDOPTS: --color=yes --tb=short -v -x --maxfail=5 --disable-warnings --no-cov-on-fail
        working-directory: ./snowpark-checkpoints-collectors

      - name: Snowflake Schema Cleanup
        if: always()
        shell: pwsh
        run: |
          $schemaId = $env:SNOWFLAKE_CONNECTIONS_MYCONNECTION_SCHEMA_ID_WIN
          Write-Output "Schema ID: $schemaId"
          if (-not [string]::IsNullOrWhiteSpace($schemaId)) {
            snow sql -q "DROP SCHEMA IF EXISTS `"$schemaId`" CASCADE;" -c myconnection
          }
        env:
          SNOWFLAKE_CONNECTIONS_MYCONNECTION_SCHEMA_ID_WIN: ${{ env.SNOWFLAKE_CONNECTIONS_MYCONNECTION_SCHEMA_ID_WIN }}

      - name: Restore Windows Firewall
        if: always()
        shell: pwsh
        run: |
          Write-Host "Restoring Windows Firewall..."
          try {
            netsh advfirewall set allprofiles state on
            Write-Host "Windows Firewall restored"
          } catch {
            Write-Host "Could not restore firewall settings"
          }
