{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cd1819-fc7d-422e-916f-4ef8fc180bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame as SparkDataFrame\n",
    "import os\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"DataFrame-Operations\").getOrCreate()\n",
    "import datetime\n",
    "start = datetime.datetime.now()\n",
    "try:\n",
    "    __file__ = __file__ \n",
    "except:\n",
    "    __file__ = os.path.join(os.getcwd(), 'DataFrame_Operations.ipynb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7e24a1-861d-4929-b3bd-5683f5bc5c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the synthetic data into a DataFrame\n",
    "import os\n",
    "directory = os.path.dirname(__file__)\n",
    "data_file_path = os.path.join( directory, \"../\" , \"data/stocks.txt\")\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType\n",
    "\n",
    "schema = StructType(\n",
    "  [\n",
    "    StructField(\"id\", IntegerType()),\n",
    "    StructField(\"name\", StringType()),\n",
    "    StructField(\"category\", StringType()),\n",
    "    StructField(\"quantity\", IntegerType()),\n",
    "    StructField(\"price\", DoubleType()),\n",
    "  ]\n",
    ")\n",
    "\n",
    "df = spark.read.schema(schema).csv(data_file_path, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdacf979-c105-4f65-9f28-4efc4c88c07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display schema of DataFrame\n",
    "df.printSchema()\n",
    "\n",
    "# Show the initial DataFrame\n",
    "print(\"Initial DataFrame:\")\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43e6f84-16f0-4f6f-a7cf-a3de49d6ea53",
   "metadata": {},
   "source": [
    "### Select: Choose specific columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7552160c-a792-4817-ab7c-117d5440a52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select specific columns\n",
    "selected_columns = df.select(\"id\", \"name\", \"price\")\n",
    "print(\"Selected Columns:\")\n",
    "selected_columns.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba2c8d3-1e26-456d-94e3-6afac5a4a4a7",
   "metadata": {},
   "source": [
    "### Filter: Apply conditions to filter rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0561ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg, round as round_, col, max as max_\n",
    "\n",
    "def filter_products():\n",
    "  items_df = df.filter(df.name.rlike(\"Headphones$\") | (df.name == \"iPhone\"))\n",
    "  total_items  = round(items_df.count() * 0.10)\n",
    "  combo_discount_df = spark.createDataFrame(items_df.orderBy(col(\"price\").asc()).take(total_items)).groupBy().agg(max_(\"price\").alias(\"two_items_discount\"))\n",
    "  inventory = (items_df.groupBy(\"name\").agg(round_(avg(\"price\") * 0.15).alias(\"discount\")).select([col('name').alias('inv_name'), col('discount')])\n",
    "               .join(combo_discount_df))\n",
    "  inventory.show()\n",
    "  discounts_df = items_df.join(inventory, on=(items_df.name == inventory.inv_name)).drop('inv_name') \n",
    "  return discounts_df\n",
    "  \n",
    "\n",
    "filtered_data = filter_products()\n",
    "\n",
    "from snowflake.snowpark_checkpoints_collector import collect_dataframe_checkpoint\n",
    "collect_dataframe_checkpoint(filtered_data, checkpoint_name=\"filtered_data\", sample=0.1, file_dir=os.path.dirname(__file__))\n",
    "\n",
    "total_price = filtered_data.select(avg(filtered_data.price))\n",
    "\n",
    "filtered_data.show()\n",
    "filtered_data.select(\"name\").distinct().show()\n",
    "total_price.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1e0a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_sport_item(products_df):  \n",
    "  products_df = products_df.repartition(5, df.name)\n",
    "  result = products_df.where(products_df.category == 'Sports')\n",
    "  result = result.orderBy(\"price\")\n",
    "  return spark.createDataFrame([result.first()]) \n",
    "\n",
    "first_sport_df = first_sport_item(df)\n",
    "\n",
    "first_sport_df.show()\n",
    "\n",
    "from snowflake.snowpark_checkpoints_collector import collect_dataframe_checkpoint\n",
    "collect_dataframe_checkpoint(first_sport_df, checkpoint_name=\"first_sport_item\", sample=1.0, file_dir=os.path.dirname(__file__))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc8c608-2fa3-4565-849b-ebc2b62025fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows based on a condition\n",
    "filtered_data = df.filter(df.quantity > 20)\n",
    "print(\"Filtered Data:\", filtered_data.count())\n",
    "filtered_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18283acf-69eb-4140-a4dd-273c9eb5eafd",
   "metadata": {},
   "source": [
    "### GroupBy: Group data based on specific columns \n",
    "### Aggregations: Perform functions like sum, average, etc., on grouped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d2db93-de0e-4707-81a8-3cfbb84dcf3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GroupBy and Aggregations\n",
    "grouped_data = df.groupBy(\"category\").agg({\"quantity\": \"sum\", \"price\": \"avg\"})\n",
    "print(\"Grouped and Aggregated Data:\")\n",
    "grouped_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a233a823-73b1-404f-b8e0-90ee5e78c4e4",
   "metadata": {},
   "source": [
    "### Join: Combine multiple DataFrames based on specified columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494f996e-57ee-44c0-82d8-814ee777653e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join with another DataFrame\n",
    "df2 = df.select(\"id\", \"category\").limit(10)\n",
    "joined_data = df.join(df2, \"id\", \"inner\")\n",
    "print(\"Joined Data:\")\n",
    "joined_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e71e549-8194-4a95-ae3a-9db0a6afa5dc",
   "metadata": {},
   "source": [
    "### Sort: Arrange rows based on one or more columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133ab21b-84ca-48e7-a8bf-8c7e487e455c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by a column\n",
    "sorted_data = df.orderBy(\"price\")\n",
    "print(\"Sorted Data:\")\n",
    "sorted_data.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a0a80e-dc5c-4569-ade9-e209560eccb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by a column desc\n",
    "from pyspark.sql.functions import col, desc\n",
    "sorted_data = df.orderBy(col(\"price\").desc(), col(\"id\").desc())\n",
    "print(\"Sorted Data Descending:\")\n",
    "sorted_data.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67580e00-93f8-4579-9972-fc64a0654366",
   "metadata": {},
   "source": [
    "### Distinct: Get unique rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744e6638-5689-4509-9df4-4abb03cd9e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get distinct product category\n",
    "distinct_rows = df.select(\"category\").distinct()\n",
    "print(\"Distinct Product Categories:\")\n",
    "distinct_rows.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c847aac-dcde-4589-aef7-c0ec95c2f80f",
   "metadata": {},
   "source": [
    "### Drop: Remove specified columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d4afa3-20b5-4299-931a-0fba2655b509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns\n",
    "dropped_columns = df.drop(\"quantity\", \"category\")\n",
    "print(\"Dropped Columns:\")\n",
    "dropped_columns.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc28820-f951-4c1e-99bc-e56ff434cc11",
   "metadata": {},
   "source": [
    "### WithColumn: Add new calculated columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb391702-9e54-4e3d-822b-0d0f1d0d08e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new calculated column\n",
    "df_with_new_column = df.withColumn(\"revenue\", df.quantity * df.price)\n",
    "print(\"DataFrame with New Column:\")\n",
    "df_with_new_column.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2138074-68c4-403b-aac6-41fee4595417",
   "metadata": {},
   "source": [
    "### Alias: Rename columns for better readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669657f4-63a0-48f6-bc9e-a1bebeae466e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns using alias\n",
    "df_with_alias = df.withColumnRenamed(\"price\", \"product_price\")\n",
    "print(\"DataFrame with Aliased Column:\")\n",
    "df_with_alias.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a40b892",
   "metadata": {},
   "outputs": [],
   "source": [
    "end = datetime.datetime.now()\n",
    "\n",
    "print((end - start))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
